{
    "add_prefix_space": false,
    "added_tokens_decoder": {
      "50280": {
        "content": "<|endoftext|>",
        "lstrip": false,
        "normalized": false,
        "rstrip": false,
        "single_word": false,
        "special": true
      }
    },
    "bos_token": "<|endoftext|>",
    "clean_up_tokenization_spaces": false,
    "eos_token": "<|endoftext|>",
    "extra_special_tokens": {},
    "model_max_length": 1000000000000000019884624838656,
    "tokenizer_class": "GPT2Tokenizer",
    "unk_token": "<|endoftext|>"
  }